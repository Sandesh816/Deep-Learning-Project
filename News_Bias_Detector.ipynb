{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sandesh816/Deep-Learning-Project/blob/main/News_Bias_Detector.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NEWS BIAS DETECTOR**"
      ],
      "metadata": {
        "id": "CdpFzeeMBVXw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oqxH1bVBA8oF"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load Data**"
      ],
      "metadata": {
        "id": "tN4JU0GODxr4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filename = \"allsides_balanced_news_headlines-texts.csv\"\n",
        "\n",
        "import requests\n",
        "url = 'https://raw.githubusercontent.com/irgroup/Qbias/refs/heads/main/allsides_balanced_news_headlines-texts.csv'\n",
        "res = requests.get(url, allow_redirects=True)\n",
        "with open(filename,'wb') as file:\n",
        "    file.write(res.content)\n",
        "\n",
        "df = pd.read_csv(filename)\n",
        "print(\"Shape:\", df.shape)\n",
        "print(df.head(10))\n",
        "print(f\"Columns: {df.columns}\")"
      ],
      "metadata": {
        "id": "2TmoHnbYB3Ev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Drop 'Unnamed: 0' column and reset index**"
      ],
      "metadata": {
        "id": "FE5DVEgmD5fX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop(columns= [\"Unnamed: 0\"], inplace = True)\n",
        "df.reset_index(drop = True, inplace = True)\n",
        "print(df.columns)"
      ],
      "metadata": {
        "id": "DKzAkVVMCJUM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.head())"
      ],
      "metadata": {
        "id": "ywWso0KoCK8z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Exploration**"
      ],
      "metadata": {
        "id": "6euwtPJbD_Mt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Explore number of labeled articles in each bucket\n",
        "left_df = df[df[\"bias_rating\"] == \"left\"]\n",
        "right_df = df[df[\"bias_rating\"] == \"right\"]\n",
        "center_df = df[df[\"bias_rating\"] == \"center\"]\n",
        "\n",
        "print(f\"Left: {left_df.shape}\")\n",
        "print(f\"Right: {right_df.shape}\")\n",
        "print(f\"Center: {center_df.shape}\")"
      ],
      "metadata": {
        "id": "AyUYvaGVEhKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Explore word counts across articles\n",
        "left_word_count = sum(left_df[\"text\"].fillna(\"\").str.split().apply(len))\n",
        "right_word_count = sum(right_df[\"text\"].fillna(\"\").str.split().apply(len))\n",
        "center_word_count = sum(center_df[\"text\"].fillna(\"\").str.split().apply(len))\n",
        "\n",
        "print(\"Left Leaning Articles Word Count:\", left_word_count)\n",
        "print(\"Right Leaning Articles Word Count:\", right_word_count)\n",
        "print(\"Center Leaning Articles Word Count:\", center_word_count)"
      ],
      "metadata": {
        "id": "Tg02A2mrE1Cw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is a discrepancy in the total word count of the articles labeled left, right, and center"
      ],
      "metadata": {
        "id": "Y8OO_T3UGTjW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyze the average lengths of left, right, and center leaning articles to check quality of dataset\n",
        "average_length_left = left_word_count / left_df.shape[0]\n",
        "average_length_right = right_word_count / right_df.shape[0]\n",
        "average_length_center = center_word_count / center_df.shape[0]\n",
        "\n",
        "print(\"Average length of left-leaning articles:\", average_length_left)\n",
        "print(\"Average length of right-leaning articles:\", average_length_right)\n",
        "print(\"Average length of center-leaning articles:\", average_length_center)"
      ],
      "metadata": {
        "id": "k-J0cZv0adCJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Explore tag lengths across articles\n",
        "left_tags_count = sum(left_df[\"tags\"].fillna(\"\").str.split(\",\").apply(len))\n",
        "right_tags_count = sum(right_df[\"tags\"].fillna(\"\").str.split(\",\").apply(len))\n",
        "center_tags_count = sum(center_df[\"tags\"].fillna(\"\").str.split(\",\").apply(len))\n",
        "\n",
        "print(\"Left Leaning Articles Tags Count:\", left_tags_count)\n",
        "print(\"Right Leaning Articles Tags Count:\", right_tags_count)\n",
        "print(\"Center Leaning Articles Tags Count:\", center_tags_count)"
      ],
      "metadata": {
        "id": "YF6iMV0DFf1R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tags counts are closer"
      ],
      "metadata": {
        "id": "fJ_U8wlAG0y4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Preparation**"
      ],
      "metadata": {
        "id": "rgb06eQ4FjfG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert words to lowercase in all columns\n",
        "df = df.map(lambda x: x.lower() if isinstance(x, str) else x)\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "z-nFEhKYDlJ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Shuffle df and split into X and Y\n",
        "# all_indices = np.arange(df.shape[0])\n",
        "# np.random.shuffle(all_indices)\n",
        "\n",
        "# test_size = int(0.2 * df.shape[0])\n",
        "# validation_size = int(0.1 * df.shape[0])\n",
        "\n",
        "# test_indices = all_indices[: test_size]\n",
        "# validation_indices = all_indices[test_size: test_size + validation_size]\n",
        "# train_indices = all_indices[test_size + validation_size: ]\n",
        "\n",
        "# X_train = df.iloc[train_indices].drop(columns = [\"bias_rating\"]).reset_index(drop = True)\n",
        "# X_validation = df.iloc[validation_indices].drop(columns = [\"bias_rating\"]).reset_index(drop = True)\n",
        "# X_test = df.iloc[test_indices].drop(columns = [\"bias_rating\"]).reset_index(drop = True)\n",
        "\n",
        "# y_train = df.iloc[train_indices][\"bias_rating\"].reset_index(drop = True)\n",
        "# y_validation = df.iloc[validation_indices][\"bias_rating\"].reset_index(drop = True)\n",
        "# y_test = df.iloc[test_indices][\"bias_rating\"].reset_index(drop = True)\n",
        "\n",
        "# print(f\"X_train shape: {X_train.shape}\")\n",
        "# print(f\"X_validation shape: {X_validation.shape}\")\n",
        "# print(f\"X_test shape: {X_test.shape}\")\n",
        "# print(f\"y_train shape: {y_train.shape}\")\n",
        "# print(f\"y_validation shape: {y_validation.shape}\")\n",
        "# print(f\"y_test shape: {y_test.shape}\")\n",
        "\n",
        "# print(X_train.head())\n",
        "# print(y_train.head())"
      ],
      "metadata": {
        "id": "jy8XlaZtKBHs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Shuffle df and split into X and Y\n",
        "# df = df.sample(frac = 1) ## don't really need it as train_test_split will shuffle all rows\n",
        "X = df[['title', 'heading', 'text']]\n",
        "y = df[\"bias_rating\"]\n",
        "print(X.head())\n",
        "print(y.head())"
      ],
      "metadata": {
        "id": "VU1Ook-LtI_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split into training, testing with even ratio between articles from each side (stratify)\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42, shuffle= True)\n",
        "X_test, X_valid, y_test, y_valid = train_test_split(X_test, y_test, test_size=0.5, stratify=y_test, random_state=42, shuffle= True)\n",
        "print(f\"X_train shape: {X_train.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}\")\n",
        "print(f\"X_valid shape: {X_valid.shape}\")\n",
        "print(f\"y_train shape: {y_train.shape}\")\n",
        "print(f\"y_test shape: {y_test.shape}\")\n",
        "print(f\"y_valid shape: {y_valid.shape}\")\n",
        "\n",
        "print(X_train.head(2))\n",
        "print(y_train.head(2))"
      ],
      "metadata": {
        "id": "HJnQ6u-qKY1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Resetting indices"
      ],
      "metadata": {
        "id": "Vnkpz1bxvrmj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = X_train.reset_index(drop = True)\n",
        "X_test = X_test.reset_index(drop = True)\n",
        "X_valid = X_valid.reset_index(drop = True)\n",
        "y_train = y_train.reset_index(drop = True)\n",
        "y_test = y_test.reset_index(drop = True)\n",
        "y_valid = y_valid.reset_index(drop = True)\n",
        "\n",
        "print(X_train.head(2))\n",
        "print(y_train.head(2))"
      ],
      "metadata": {
        "id": "_A24a_owvTHH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**Milestone 2**\n",
        "\n"
      ],
      "metadata": {
        "id": "IGcLqjcmKT5t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import huggingface_hub\n",
        "from transformers import AutoTokenizer, TFBertModel"
      ],
      "metadata": {
        "id": "6dwDwSxBGwla"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We will use the BERT model as our baseline model\n",
        "model_name = \"bert-base-uncased\"\n",
        "model = TFBertModel.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "5oJ7-cy7HSU-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the first article's text for sanity check\n",
        "test_tokens = tokenizer.tokenize(X_train[\"text\"][0])\n",
        "test_token_ids = tokenizer.convert_tokens_to_ids(test_tokens)\n",
        "print(test_tokens)\n",
        "print(test_token_ids)"
      ],
      "metadata": {
        "id": "Cw_VbecNBXwT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyze the token count across the articles after tokenizing all text\n",
        "left_token_count = sum(len(tokenizer.tokenize(str(text))) for text in left_df[\"text\"] if pd.notna(text))\n",
        "right_token_count = sum(len(tokenizer.tokenize(str(text))) for text in right_df[\"text\"] if pd.notna(text))\n",
        "center_token_count = sum(len(tokenizer.tokenize(str(text))) for text in center_df[\"text\"] if pd.notna(text))\n",
        "\n",
        "print(\"Left-leaning articles token count:\", left_token_count)\n",
        "print(\"Right-leaning articles token count:\", right_token_count)\n",
        "print(\"Center-leaning articles token count:\", center_token_count)"
      ],
      "metadata": {
        "id": "kUEJToAtZtkl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Trying BERT Baseline Model**"
      ],
      "metadata": {
        "id": "yISJdkW1ELjm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use the process described by Keras to train on a portion of our data: https://keras.io/keras_hub/api/models/bert/bert_text_classifier/"
      ],
      "metadata": {
        "id": "wdBevr1vJCiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import keras_hub\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "\n",
        "# Grab our train data\n",
        "features = X_train.copy()\n",
        "features = features[:1000]\n",
        "features = list(features[\"text\"].astype(str))\n",
        "\n",
        "# Grab our train labels and map string labels to numerical labels\n",
        "label_mapping = {'left': 0, 'center': 1, 'right': 2}\n",
        "labels = np.array([label_mapping[label] for label in y_train])\n",
        "labels = labels[:1000]\n",
        "\n",
        "# Pretrained classifier.\n",
        "classifier = keras_hub.models.BertTextClassifier.from_preset(\n",
        "    \"bert_base_en_uncased\",\n",
        "    num_classes=3,\n",
        ")\n",
        "classifier.fit(x=features, y=labels, batch_size=2)\n",
        "classifier.predict(x=features, batch_size=2)\n",
        "\n",
        "# Re-compile (e.g., with a new learning rate).\n",
        "classifier.compile(\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer=\"adam\",\n",
        "    jit_compile=True,\n",
        ")\n",
        "# Access backbone programmatically (e.g., to change `trainable`).\n",
        "classifier.backbone.trainable = False\n",
        "# Fit again.\n",
        "classifier.fit(x=features, y=labels, batch_size=2)\n"
      ],
      "metadata": {
        "id": "uQozOsGpI-B6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "BERT Baseline Model Evaluation"
      ],
      "metadata": {
        "id": "DKccB_VLSnnc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare data for evaluation\n",
        "X_test_list = list(X_test[\"text\"].astype(str))  # Convert X_test to a list of strings\n",
        "y_test_mapped = np.array([label_mapping[label] for label in y_test])  # Map y_test labels\n",
        "\n",
        "# Create a tf.data.Dataset for evaluation\n",
        "eval_dataset = tf.data.Dataset.from_tensor_slices((X_test_list, y_test_mapped)).batch(2)\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = classifier.evaluate(eval_dataset) # Use classifier instead of model\n",
        "print(f\"Loss: {loss}, Accuracy: {accuracy}\")\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Get predictions\n",
        "preds = classifier.predict(eval_dataset) # Use classifier instead of model\n",
        "y_pred = np.argmax(preds, axis=1)  # Get predicted labels\n",
        "\n",
        "# Get true labels\n",
        "y_true = np.concatenate([y for x, y in eval_dataset], axis=0)\n",
        "\n",
        "print(classification_report(y_true, y_pred, target_names=['left', 'center', 'right']))"
      ],
      "metadata": {
        "id": "JaVG2yGIETs8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Trying an LSTM custom model**"
      ],
      "metadata": {
        "id": "wZIsnOK7aU3b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also wanted to experiment with trying out using an LSTM that we built on our data."
      ],
      "metadata": {
        "id": "VwSgxNnaHJGE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import keras_hub\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import huggingface_hub\n",
        "from transformers import AutoTokenizer, TFBertModel\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense"
      ],
      "metadata": {
        "id": "dmV4a2qx9BLB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "label_map = {\"left\": 0, \"right\": 1, \"center\": 2}\n",
        "X_text = (df[\"title\"].fillna(\"\") + df[\"heading\"].fillna(\"\") + df[\"text\"].fillna(\"\")).to_list() # now, we get a list of strings as X_text\n",
        "y = df[\"bias_rating\"].str.lower().map(label_map)\n",
        "\n",
        "max_length = 600\n",
        "X = [\n",
        "    tokenizer.encode(x, max_length=max_length, truncation=True, add_special_tokens= True) for x in X_text\n",
        "]\n",
        "\n",
        "X_pad = pad_sequences(X, maxlen=max_length, padding='post', truncating='post').astype(\"int32\") # padding will allow us to send batches as tensor\n",
        "y_np  = y.to_numpy(dtype=\"int32\")\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_pad, y_np, test_size=0.2, stratify=y_np)"
      ],
      "metadata": {
        "id": "aTzk2e2Wy5pN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating the train and validation dataset\n",
        "BATCH = 32\n",
        "train_ds = (\n",
        "    tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
        "      .shuffle(10_000)\n",
        "      .batch(BATCH)\n",
        "      .prefetch(tf.data.AUTOTUNE)\n",
        ")\n",
        "\n",
        "val_ds = (\n",
        "    tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
        "      .batch(BATCH)\n",
        "      .prefetch(tf.data.AUTOTUNE)\n",
        ")"
      ],
      "metadata": {
        "id": "qV-FoFmP-Ifu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# building our bidirectional LSTM model (embedding -> dropout -> biD -> dropout -> biD -> Dense)\n",
        "vocab_size = tokenizer.vocab_size\n",
        "num_classes = 3\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=128))\n",
        "model.add(tf.keras.layers.Dropout(0.3))\n",
        "model.add(tf.keras.layers.Bidirectional(LSTM(64, return_sequences=True)))\n",
        "model.add(tf.keras.layers.Dropout(0.3))\n",
        "model.add(tf.keras.layers.Bidirectional(LSTM(64)))\n",
        "model.add(tf.keras.layers.Dense(num_classes, activation='softmax'))\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "es = tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)\n",
        "\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=10,\n",
        "    callbacks=[es]\n",
        ")"
      ],
      "metadata": {
        "id": "Kj_TtbqQ_ATY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Above are our results from our initial test using an LSTM. The results seem to show that the model is indeed training, however the validation accuracy is not changing, which shows something is off. The model needs to be trained for many more epochs to achieve better accuracy."
      ],
      "metadata": {
        "id": "nZYT2iVcFQW-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluate on Testing Set**"
      ],
      "metadata": {
        "id": "Iba5lGbyGV98"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get predictions\n",
        "pred_probs = model.predict(X_val, batch_size=32)\n",
        "\n",
        "y_pred = np.argmax(pred_probs, axis=1)\n",
        "y_true = y_val\n",
        "\n",
        "print(classification_report(y_true, y_pred,\n",
        "                            target_names=['left', 'center', 'right']))"
      ],
      "metadata": {
        "id": "0rhRf3B3fpPv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create requirements.txt**"
      ],
      "metadata": {
        "id": "SghsByraHV1c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip freeze > requirements.txt"
      ],
      "metadata": {
        "id": "fwpbnfHRHyjs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UX7JctlQElGs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}